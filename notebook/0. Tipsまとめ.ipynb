{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Optim\" data-toc-modified-id=\"Optim-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Optim</a></span><ul class=\"toc-item\"><li><span><a href=\"#Optimizerの種類\" data-toc-modified-id=\"Optimizerの種類-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Optimizerの種類</a></span></li><li><span><a href=\"#どれを使うのか?\" data-toc-modified-id=\"どれを使うのか?-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>どれを使うのか?</a></span></li><li><span><a href=\"#Learning-rateの調整\" data-toc-modified-id=\"Learning-rateの調整-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Learning rateの調整</a></span></li></ul></li><li><span><a href=\"#重みの初期値\" data-toc-modified-id=\"重みの初期値-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>重みの初期値</a></span></li><li><span><a href=\"#Batch-Normalization\" data-toc-modified-id=\"Batch-Normalization-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Batch Normalization</a></span></li><li><span><a href=\"#畳み込みニューラルネットワーク\" data-toc-modified-id=\"畳み込みニューラルネットワーク-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>畳み込みニューラルネットワーク</a></span><ul class=\"toc-item\"><li><span><a href=\"#畳み込み層\" data-toc-modified-id=\"畳み込み層-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>畳み込み層</a></span></li></ul></li><li><span><a href=\"#重要なネットワーク構造\" data-toc-modified-id=\"重要なネットワーク構造-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>重要なネットワーク構造</a></span><ul class=\"toc-item\"><li><span><a href=\"#VGG\" data-toc-modified-id=\"VGG-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>VGG</a></span></li><li><span><a href=\"#GoogLeNet\" data-toc-modified-id=\"GoogLeNet-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>GoogLeNet</a></span></li><li><span><a href=\"#ResNet\" data-toc-modified-id=\"ResNet-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>ResNet</a></span></li><li><span><a href=\"#PyTorchで利用可能なモデル\" data-toc-modified-id=\"PyTorchで利用可能なモデル-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>PyTorchで利用可能なモデル</a></span></li></ul></li><li><span><a href=\"#ユーティリティ関係\" data-toc-modified-id=\"ユーティリティ関係-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>ユーティリティ関係</a></span><ul class=\"toc-item\"><li><span><a href=\"#結果の保存\" data-toc-modified-id=\"結果の保存-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>結果の保存</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optim\n",
    "\n",
    "**使い方:** \n",
    "\n",
    "- `optimizer`をインスタンス化し、勾配情報からparametersを更新する。\n",
    "- すべてのoptimizerは`step`を実装しており、これを実行すればパラメータが更新される\n",
    "    - `optimizer.step()`を実行\n",
    "    - `backward()`を実行して勾配を計算したあと一度だけ実行する。\n",
    "    - Conjugate GradientやLBFGSなどの最適化アルゴリズムを使用する場合は異なる。(割愛)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizerの種類\n",
    "\n",
    "- `Adadelta`\n",
    "- `Adagrad`\n",
    "    - パラメータ一つ一つに対して個別のlearning rateを適応させる\n",
    "    - \"適応\": AdaptiveからAdagrad\n",
    "    - 値が大きく更新されたパラメータの学習率は小さくしていく\n",
    "    - 具体的に言うと、これまでの勾配の2乗の値が蓄積されていき、勾配をその平方根で除して更新する\n",
    "    - パラメータ更新の度に勾配の2乗の値は蓄積されて無限大になるので、やがて更新値は0となる\n",
    "        - これを改善したのがRMSprop\n",
    "- `Adam`\n",
    "    - Adaptive momentum estimation\n",
    "    - 要は、Adagrad + RMSprop(Momenumも含む?)\n",
    "    - 詳細は、*Kingma*らの原著論文を参照して(arXiv:1412.6980)\n",
    "- `AdamW`\n",
    "- `SparseAdam`\n",
    "- `Adamax`\n",
    "- `ASGD`\n",
    "- `LBFGS`\n",
    "- `RMSprop`\n",
    "    - \"Adagrad\"での勾配情報の二乗値の蓄積部分を指数移動平均に変更したもの\n",
    "    - 直近の更新情報がより重み付けされることになる\n",
    "    - Adagradと違ってすべてを蓄積するわけではないので、更新値が0となることはない\n",
    "- `Rprop`\n",
    "- `SGD`\n",
    "    - 確率的勾配降下法\n",
    "    - データからミニバッチを無作為に取り出しそのデータから勾配降下法を用いてパラメータ更新\n",
    "    - なので\"確率的\"という名前がつく\n",
    "    - データ全体に対して行う勾配降下法は\"バッチ勾配降下法\"と呼称されることも\n",
    "    - 勾配の方向が極小点を指していないので、パラメータ更新が非常に非効率\n",
    "    - その結果として、損失関数は更新の過程で大きく振動するようすがみられる\n",
    "    - Pytorchでは本来のSGDというよりは、学習率が固定されたOptimizerという意味?\n",
    "    - つまり、データの入力は1セットではなく、ミニバッチが前提となっている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### どれを使うのか?\n",
    "\n",
    "- 決定的な方法は無いし、選定の指標も無い(と思う)\n",
    "- 単純にSGDが使われる場合も多い\n",
    "- ただ、一般的にはSGDよりも他の手法の方が収束は速いし精度が出る傾向にある\n",
    "- Adamは結構やるやつ\n",
    "- それ以外でもはまるケースはある\n",
    "\n",
    "---\n",
    "**結論: まずはAdam試しとけ。精度が出ない、もしくは時間があるなら他の手法も試してみろ。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rateの調整\n",
    "\n",
    "`torch.optim.lr_scheduler`を使用してエポックごとのlearning rateの調整が可能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みの初期値\n",
    "\n",
    "**重みの初期値の選定は非常に重要な問題**\n",
    "\n",
    "不適切な初期値を利用すると、勾配消失が起こり、学習がまったく進まなくなってしまう。\n",
    "(損失関数が全然減らなくなってしまう)\n",
    "\n",
    "- Xavierの初期値\n",
    "    - 活性化関数がシグモイド関数の場合に有効な手法\n",
    "- Heの初期値\n",
    "    - 活性化関数がReLU関数の場合に有効な手法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "活性化後の値に偏りがあると、学習がうまくいかない。(勾配消失、など)\n",
    "\n",
    "強制的に値に分布をもたせてやれば、学習が早く進むのでは?というアイデアが、**Batch Normalization**\n",
    "\n",
    "※Courseraのコースでは、Week3でこの内容の講義があった。\n",
    "\n",
    "---\n",
    "[利点]\n",
    "\n",
    "- 学習が速い\n",
    "- 初期値依存があまりない\n",
    "    - Batch Normによって調整されるので、初期値の分布を気にする必要性が小さくなる\n",
    "- 化学臭が抑制される\n",
    "    - その他の過学習抑制方法を使用しなくてもよくなる\n",
    "    \n",
    "---\n",
    "\n",
    "- **通常の正規化(Normalization)**: 入力データに対して平均を0、分散を1に調整する\n",
    "- **Batch Normalization**: すべての隠れ層への入力に対してNormalizationを行う。また、その後にバイアスの加算とスケーリングが行われる。\n",
    "    - 通常は、活性化前にNormalizationを行うことが一般的\n",
    "    - 各層の活性はミニバッチごとに変わるので、ミニバッチごとにNormalizationを行う。\n",
    "    - 重みがある層(FC層など)と活性化層(ReLUなど)の間に新しくBatch Norm層が入るイメージ\n",
    "    - 当然、backpropのときにもこれを考慮して計算する必要がある\n",
    "\n",
    "---\n",
    "[Batch Normの計算]\n",
    "\n",
    "$$\n",
    "Z_{\\mathrm{norm}}^{(i)} = \\frac{Z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "これは、平均が0で分散が1のパラメータになる。\n",
    "\n",
    "**ただし、隠れ層の出力では、この分布(平均0、分散1)が常に適しているとは限らない。**\n",
    "なので、以下のように、任意の分布となるようにパラメータを変換してもいい。\n",
    "\n",
    "$$\n",
    "\\tilde{Z}^{(i)} = \\gamma Z_{\\mathrm{norm}}^{(i)} + \\beta\n",
    "$$\n",
    "\n",
    "$\\tilde{Z}^{(i)}$ は平均$\\beta$、分散$\\gamma$の分布をもつことになる。\n",
    "\n",
    "これを活性化関数にぶちこみ、この層からの出力が得られる。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ミニバッチサイズ\n",
    "    - ハイパーパラメータ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 畳み込みニューラルネットワーク\n",
    "\n",
    "主に以下の層から構成される\n",
    "\n",
    "- Convolution layer\n",
    "- Pooling layer\n",
    "- Full connected(FC) layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 畳み込み層\n",
    "\n",
    "- **出力される画像(2次元データ)のサイズ**: カーネルサイズやpadding, strideの大きさによって一意に決まる\n",
    "- **出力されるデータのチャンネル数**: 畳み込み層のチャンネル数。これは任意の値に指定する。\n",
    "\n",
    "一般的には、2次元方向のサイズはどんどん小さくしていき、チャンネル数を増やしていく、というネットワーク構造が多い。\n",
    "(つまり、2次元の大きな入力データを、畳み込み層をかましながら1次元のデータにしていく、という流れ)\n",
    "その後、Affine層をかまして出力を得る。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "とある畳み込み層について\n",
    "\n",
    "- kernelサイズ: $f$\n",
    "    - kernelのチャンネル数は、必ずこの層への入力データのチャンネル数に等しくする\n",
    "- paddingの数: $p$\n",
    "- strideの数: $s$\n",
    "\n",
    "これらはネットワーク構築の際に設定する。\n",
    "\n",
    "※ kernelの中身は学習の対象。\n",
    "\n",
    "この畳み込み層に対して、$n \\times h \\times w$のデータが入力され、\n",
    "$n^{'} \\times h{'} \\times w{'}$\n",
    "のデータが出力される、とする。\n",
    "\n",
    "このうち、$h^{'} \\times w^{'}$は$f, p, s$と$h, w$によって決まる。\n",
    "\n",
    "$$\n",
    "h^{'} \\times w^{'} = \n",
    "\\left( \\lfloor \\frac{h + 2p - f}{s} \\rfloor + 1 \\right) \n",
    "\\times\n",
    "\\left( \\lfloor \\frac{w + 2p - f}{s} \\rfloor + 1 \\right)\n",
    "$$\n",
    "\n",
    "これを計算するためには入力データのチャンネル分のカーネルが必要となる。\n",
    "(この場合は、$n \\times f \\times f$のカーネルを、\n",
    "入力データ$n \\times h \\times w$に対して適用し、\n",
    "$1 \\times h^{'} \\times w^{'}$が得られる)\n",
    "\n",
    "このように、1つのカーネルからは1チャンネルの二次元データが得られることになる。\n",
    "出力チャンネル数は、このカーネルの個数によって決まる。\n",
    "\n",
    "$n^{'}$については、ネットワークを構築する際に決める。\n",
    "(一般的に、ネットワークが深くなるにつれて、$n^{'}$の値を大きくする。)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重要なネットワーク構造\n",
    "\n",
    "- VGG\n",
    "- GoogLeNet\n",
    "- ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG\n",
    "\n",
    "- PoolingやStride、kernelサイズ、フィルター数は固定しておく(ユーザーは調整しない)\n",
    "- シンプルな構造なので学習しやすく、応用がききやすい"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogLeNet\n",
    "\n",
    "- Inception構造を基本としたネットワーク構造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorchで利用可能なモデル\n",
    "\n",
    "- `AlexNet`\n",
    "- `VGG`\n",
    "- `ResNet`\n",
    "- `SqueezeNet`\n",
    "- `DenseNet`\n",
    "- `Inception v3`\n",
    "- `GoogLeNet`\n",
    "- `ShuffleNet v2`\n",
    "- `MobileNet v2`\n",
    "- `ResNeXt`\n",
    "- `Wide ResNet`\n",
    "- `MNASNet`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ユーティリティ関係"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 結果の保存\n",
    "\n",
    "- ベストプラクティスは、`torch.save`を使用して、ネットワークの`state_dict()`を保存する。\n",
    "- ロードする場合は、ネットワークをインスタンス化して、`load_state()`を実行\n",
    "\n",
    "詳細は、\n",
    "[ここ](https://pytorch.org/docs/stable/notes/serialization.html)\n",
    "に記載がある。"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
