{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#インストールについて\" data-toc-modified-id=\"インストールについて-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>インストールについて</a></span></li><li><span><a href=\"#What-is-PyTorch\" data-toc-modified-id=\"What-is-PyTorch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is PyTorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#演算\" data-toc-modified-id=\"演算-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>演算</a></span></li></ul></li><li><span><a href=\"#AUTOGRAD:-AUTOMATIC-DIFFERENTIATION\" data-toc-modified-id=\"AUTOGRAD:-AUTOMATIC-DIFFERENTIATION-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AUTOGRAD: AUTOMATIC DIFFERENTIATION</a></span></li><li><span><a href=\"#ニューラルネットワーク\" data-toc-modified-id=\"ニューラルネットワーク-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ニューラルネットワーク</a></span><ul class=\"toc-item\"><li><span><a href=\"#かんたんなネットワークを構築する\" data-toc-modified-id=\"かんたんなネットワークを構築する-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>かんたんなネットワークを構築する</a></span></li><li><span><a href=\"#損失関数\" data-toc-modified-id=\"損失関数-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>損失関数</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストールについて\n",
    "\n",
    "公式を参考にそのままやったらかなり躓いたのでメモしておく。\n",
    "\n",
    "公式ではPyTorchやCUDAのバージョンを選択してインストール用のコマンドを生成してくれるが、\n",
    "これでやってしまうとうまくインストールできなかった。  \n",
    "(`import torch`すると、`DLL load failed`となってしまった)\n",
    "\n",
    "おそらくだけど、これを行った時点ではPyTorchの最新バージョンは1.3.0だが、\n",
    "まだWindowsに対応していないのでは?\n",
    "対策として一つ前のバージョンである1.2.0を入れることにした。\n",
    "\n",
    "また、前のバージョンである1.2.0を、これも公式のとおりにインストールしたら\n",
    "CUDA9に対応したPyTorchが入ってしまった。\n",
    "(以下のコマンド)\n",
    "\n",
    "```\n",
    "pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "```\n",
    "\n",
    "以下のページを参考にしながら、次のように行うことで、\n",
    "CUDA10を使用したPyTorch Ver1.2をインストールすることができた。\n",
    "\n",
    "```\n",
    "pip3 install https://download.pytorch.org/whl/cu100/torch-1.2.0-cp36-cp36m-win_amd64.whl\n",
    "\n",
    "pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.4.0-cp36-cp36m-win_amd64.whl\n",
    "```\n",
    "\n",
    "これをPipfileのscriptに書いておいたので、あとからでもインストールできると思う。\n",
    "\n",
    "---\n",
    "\n",
    "**参考**\n",
    "\n",
    "- [1](https://discuss.pytorch.org/t/getting-cuda-version-9-0-17-but-nvcc-shows-the-cuda-version-to-be-10-0-130/48300)\n",
    "- [2](https://drumato.hatenablog.com/entry/2019/01/13/104206)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3326, 0.6948, 0.0352],\n",
      "        [0.1532, 0.1289, 0.8538],\n",
      "        [0.1688, 0.0574, 0.1348],\n",
      "        [0.1047, 0.1282, 0.8588],\n",
      "        [0.1623, 0.9180, 0.1592]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version using in PyTorch is {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch\n",
    "\n",
    "変数やテンソルの生成、扱い方など。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x = torch.zeros(5, 3)\n",
    "x = torch.rand(5, 3)\n",
    "x = torch.randn(5, 3)\n",
    "x = torch.ones(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `empty`\n",
    "- `zeros`\n",
    "- `ones`\n",
    "- `rand`\n",
    "- `randn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演算\n",
    "\n",
    "例えば足し算は`+`演算子でもできるし、`tensor`がもっている`add`メソッドでもできる。\n",
    "\n",
    "`tensor`自身に足した結果を格納したい場合には、`postfix`に`_`をつけたメソッドを呼び出せばよい。\n",
    "足し算の場合には、`add_`メソッドとなる。\n",
    "足し算以外の他の演算メソッドでもこの法則は同様。\n",
    "\n",
    "また、`postfir`に`_like`とあるものは、入力にtensorをとり、\n",
    "その形状と同じtensorを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = \n",
      "tensor([[0.5064, 0.8020, 1.5279],\n",
      "        [1.3643, 0.9701, 1.5644],\n",
      "        [0.7919, 1.6548, 0.6321],\n",
      "        [0.1721, 1.2893, 1.3651],\n",
      "        [0.9196, 0.5909, 1.3498]])\n",
      "x = \n",
      "tensor([[0.5064, 0.8020, 1.5279],\n",
      "        [1.3643, 0.9701, 1.5644],\n",
      "        [0.7919, 1.6548, 0.6321],\n",
      "        [0.1721, 1.2893, 1.3651],\n",
      "        [0.9196, 0.5909, 1.3498]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "print(f\"x + y = \\n{x + y}\")\n",
    "\n",
    "print(f\"x = \\n{x.add_(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3157, 0.8639, 0.7970],\n",
      "        [0.5824, 0.0929, 0.1165],\n",
      "        [0.1699, 0.1246, 0.2976],\n",
      "        [0.3069, 0.2192, 0.0228],\n",
      "        [0.3056, 0.3595, 0.4127]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1st row of x:\n",
      "tensor([0.5064, 0.8020, 1.5279])\n"
     ]
    }
   ],
   "source": [
    "# numpy-likeにスライスすることも可能\n",
    "print(f\"The 1st row of x:\\n{x[0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チュートリアルでは紹介しきれない演算子が多くある。\n",
    "(例えば、transposeやslicing、代数演算など)\n",
    "\n",
    "それらの詳細は以下にすべてまとまっているので、適宜参考にすること。\n",
    "\n",
    "[docs](https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy-converted obj: [1. 1. 1. 1. 1.]\n",
      "the converted obj reference the same obejct.\n",
      "numpy-converted obj: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# tensorをndarrayに変換可能\n",
    "\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(f\"numpy-converted obj: {b}\")\n",
    "\n",
    "a.add_(1)\n",
    "\n",
    "print(f\"the converted obj reference the same obejct.\")\n",
    "print(f\"numpy-converted obj: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "\n",
    "参考ページ:[ここ](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "# out = z.mean()\n",
    "\n",
    "# print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように`requires_grad`属性を`True`にしたいtensorに対して演算を行うと、\n",
    "その演算に対する微分情報が得られる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の結果は、`out`を`x`で偏微分したもの、つまりはヤコビアン。\n",
    "(`out`は`x`の関数。)\n",
    "\n",
    "計算過程を詳説すると、以下のとおり。\n",
    "\n",
    "---\n",
    "`out`を$\\boldsymbol{o}$ と表記すると、$\\boldsymbol{o} = \\frac{1}{4} \\sum 3(\\boldsymbol{x} +2)^2$となる。\n",
    "\n",
    "これを$\\boldsymbol{x}$ で微分したものが上記となる。\n",
    "\n",
    "$\\frac{\\boldsymbol{o}}{x_i} = \\frac{3}{2}(x_i + 2)$である。$x_i = 1$のとき、これは4.5となる。\n",
    "\n",
    "よって、上記のような計算結果となる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 8.],\n",
      "        [8., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = 2 * x ** 4 + 5\n",
    "\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル$\\boldsymbol{y}$に対して、ベクトル$\\boldsymbol{x}$の各要素で偏微分したものはヤコビ行列(ヤコビアン)と呼ばれる。\n",
    "\n",
    "一般に、`torch.autograd`はヤコビアンと行列の積を求める計算エンジンである。\n",
    "\n",
    "つまり、任意のベクトル$\\boldsymbol{v}$が与えられた場合、$\\boldsymbol{v}^\\mathrm{T} \\cdot \\boldsymbol{J}$が計算される。\n",
    "\n",
    "仮に$\\boldsymbol{v}$がスカラー量$l = f(\\boldsymbol{y})$の勾配であった場合、つまり、\n",
    "$\\boldsymbol{v} = \\{ \\frac{\\partial{l}}{\\partial{y_1}},...,  \\frac{\\partial{l}}{\\partial{y_n}}\\}^\\mathrm{T}$\n",
    "である場合、**連鎖律(chain rule)** によって、\n",
    "ベクトル-ヤコビアンの積は、スカラー量$l$を$\\boldsymbol{x}$で微分したものとなる。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{J}^\\mathrm{T} \\cdot \\boldsymbol{v} = \\{ \\frac{\\partial{l}}{\\partial{x_1}},...,  \\frac{\\partial{l}}{\\partial{x_m}}\\}^\\mathrm{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "Pytorchでは`torch.nn`パッケージを用いてニューラルネットワークを構築していく。\n",
    "\n",
    "ニューラルネットワークのトレーニングの主な流れは以下のとおり。\n",
    "\n",
    "- ネットワークを構築する\n",
    "- 入力データを入れる\n",
    "- 損失を計算する\n",
    "- backprop計算\n",
    "- ネットワーク内の重みを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### かんたんなネットワークを構築する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1層目\n",
    "        # 1チャンネルの画像データを入力\n",
    "        # 出力には6チャンネルのデータとなる\n",
    "        # カーネルサイズは3x3\n",
    "        # 注: 畳み込み層では出力画像のサイズを指定しない。\n",
    "        # (サイズはカーネルサイズやPoolong層、パディングによって自動的に決まる)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        \n",
    "        # 2層目\n",
    "        # 6チャンネルのデータを入力\n",
    "        # 出力には16チェンネルのデータとなる\n",
    "        # カーネルサイズは3x3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # 3層目以降は全結合層(アフィン層とも)\n",
    "        # 入力は16チャンネルの6x6の２2次元データで、\n",
    "        # 120行の出力を出す\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        # 以降は同様\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling8(2x2)をかます\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Pooling範囲が正方であるならば数字単独でもOK\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # フラット化する\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # あとは残りのAffine層に突っ込む\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # all dimensions except the batch dimension\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward()`を定義したが、`backward`は定義していない。\n",
    "\n",
    "これについては、`forward`の処理から自動的に決まるので、\n",
    "あとは`autograd`がやってくれる。\n",
    "\n",
    "ユーザーは好きに`forward()`内の演算を定義するだけでよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parameters()`で重みを取得できる。\n",
    "\n",
    "以下では試しに1層目の重みのサイズをprintしている。\n",
    "\n",
    "1チャンネルの画像データに対して、(3x3)のカーネルを適用して、\n",
    "6チャンネルのデータを出力する重みとなっていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0416, -0.0682, -0.0028,  0.0812, -0.0448, -0.0658,  0.1451, -0.0497,\n",
      "          0.0973, -0.0796]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 適当な値で値を出力してみる。\n",
    "data_input = torch.randn(1, 1, 32, 32)\n",
    "data_out = net(data_input)\n",
    "print(data_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Caution]\n",
    "\n",
    "- `tourch.nn`はミニバッチデータのみを受け付ける\n",
    "    - 入力データの形状は`nSamples x nChannels x Height x Width`\n",
    "    - なので、上記の適当データでもshapeが(1, 1, 32, 32)としている"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数\n",
    "\n",
    "以上がニューラルネットワークを構築するまでの処理。\n",
    "構築したニューラルネットワークは、2層の畳み込み層と、3層のAffine層。\n",
    "そして、ランダムなデータを入力して、順伝播して出力を得た。\n",
    "\n",
    "このニューラルネットワークを学習させることになるが、\n",
    "そのためにはその指標である損失関数を計算する必要がある。\n",
    "\n",
    "`nn`パッケージにはいくつかの損失関数が存在する。\n",
    "最もシンプルなのが、`nn.MSELoss`(*mean square error*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4094, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "data_out = net(data_input)\n",
    "target = torch.randn(10)\n",
    "target = target.view(1, -1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# data_outがネットワークからの出力\n",
    "# targetが教師データに相当する\n",
    "loss = criterion(data_out, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x7f6cd925dda0>\n",
      "<AddmmBackward object at 0x7f6cd925def0>\n",
      "<AccumulateGrad object at 0x7f6cd925dda0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear\n",
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loss.backward()`を実行すると、計算グラフ全体が損失関数に対して偏微分され、\n",
    "計算グラフ中のすべてのテンソル(`required_grad=True`であるテンソル)は、\n",
    "勾配が蓄積されてきた`.grad`テンソルをもつことになる。\n",
    "\n",
    "backwardを計算する前に、勾配をゼロで初期化する必要がある。\n",
    "(勾配情報がそれまでの値に加算されていくので、ゼロ初期化しておかないと値がおかしくなる。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0023, -0.0053,  0.0072, -0.0006, -0.0024, -0.0043])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重みの更新について\n",
    "\n",
    "`Net`の`parameters`属性に、各層のパラメータについての情報がある。\n",
    "各`parameters`の`data`属性に実際の重み行列となっている。\n",
    "\n",
    "これを学習率と勾配から更新する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重み更新方法には様々な方法がある。\n",
    "\n",
    "- Adam\n",
    "- RMS-prop\n",
    "- SGD\n",
    "\n",
    "PyTorchでは、これらを簡易に使用できるようになっている。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "data_out = net(data_input)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(data_out, target)\n",
    "net.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
