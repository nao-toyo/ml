{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#インストールについて\" data-toc-modified-id=\"インストールについて-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>インストールについて</a></span></li><li><span><a href=\"#What-is-PyTorch\" data-toc-modified-id=\"What-is-PyTorch-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>What is PyTorch</a></span><ul class=\"toc-item\"><li><span><a href=\"#演算\" data-toc-modified-id=\"演算-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>演算</a></span></li></ul></li><li><span><a href=\"#AUTOGRAD:-AUTOMATIC-DIFFERENTIATION\" data-toc-modified-id=\"AUTOGRAD:-AUTOMATIC-DIFFERENTIATION-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>AUTOGRAD: AUTOMATIC DIFFERENTIATION</a></span></li><li><span><a href=\"#ニューラルネットワーク\" data-toc-modified-id=\"ニューラルネットワーク-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>ニューラルネットワーク</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## インストールについて\n",
    "\n",
    "公式を参考にそのままやったらかなり躓いたのでメモしておく。\n",
    "\n",
    "公式ではPyTorchやCUDAのバージョンを選択してインストール用のコマンドを生成してくれるが、\n",
    "これでやってしまうとうまくインストールできなかった。  \n",
    "(`import torch`すると、`DLL load failed`となってしまった)\n",
    "\n",
    "おそらくだけど、これを行った時点ではPyTorchの最新バージョンは1.3.0だが、\n",
    "まだWindowsに対応していないのでは?\n",
    "対策として一つ前のバージョンである1.2.0を入れることにした。\n",
    "\n",
    "また、前のバージョンである1.2.0を、これも公式のとおりにインストールしたら\n",
    "CUDA9に対応したPyTorchが入ってしまった。\n",
    "(以下のコマンド)\n",
    "\n",
    "```\n",
    "pip install torch==1.2.0 torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "```\n",
    "\n",
    "以下のページを参考にしながら、次のように行うことで、\n",
    "CUDA10を使用したPyTorch Ver1.2をインストールすることができた。\n",
    "\n",
    "```\n",
    "pip3 install https://download.pytorch.org/whl/cu100/torch-1.2.0-cp36-cp36m-win_amd64.whl\n",
    "\n",
    "pip3 install https://download.pytorch.org/whl/cu100/torchvision-0.4.0-cp36-cp36m-win_amd64.whl\n",
    "```\n",
    "\n",
    "これをPipfileのscriptに書いておいたので、あとからでもインストールできると思う。\n",
    "\n",
    "---\n",
    "\n",
    "**参考**\n",
    "\n",
    "- [1](https://discuss.pytorch.org/t/getting-cuda-version-9-0-17-but-nvcc-shows-the-cuda-version-to-be-10-0-130/48300)\n",
    "- [2](https://drumato.hatenablog.com/entry/2019/01/13/104206)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6743, 0.4999, 0.2041],\n",
      "        [0.1182, 0.4369, 0.9295],\n",
      "        [0.9445, 0.6874, 0.0416],\n",
      "        [0.2082, 0.6115, 0.8720],\n",
      "        [0.7573, 0.5505, 0.6653]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version using in PyTorch is 10.0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version using in PyTorch is {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch\n",
    "\n",
    "変数やテンソルの生成、扱い方など。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(5, 3)\n",
    "x = torch.zeros(5, 3)\n",
    "x = torch.rand(5, 3)\n",
    "x = torch.randn(5, 3)\n",
    "x = torch.ones(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `empty`\n",
    "- `zeros`\n",
    "- `ones`\n",
    "- `rand`\n",
    "- `randn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 演算\n",
    "\n",
    "例えば足し算は`+`演算子でもできるし、`tensor`がもっている`add`メソッドでもできる。\n",
    "\n",
    "`tensor`自身に足した結果を格納したい場合には、`postfix`に`_`をつけたメソッドを呼び出せばよい。\n",
    "足し算の場合には、`add_`メソッドとなる。\n",
    "足し算以外の他の演算メソッドでもこの法則は同様。\n",
    "\n",
    "また、`postfir`に`_like`とあるものは、入力にtensorをとり、\n",
    "その形状と同じtensorを返す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x + y = \n",
      "tensor([[1.4379, 1.0812, 1.1456],\n",
      "        [0.3082, 0.8464, 0.5303],\n",
      "        [1.7579, 0.9883, 0.6281],\n",
      "        [1.4171, 0.8570, 0.7017],\n",
      "        [0.1624, 0.2367, 1.3165]])\n",
      "x = \n",
      "tensor([[1.4379, 1.0812, 1.1456],\n",
      "        [0.3082, 0.8464, 0.5303],\n",
      "        [1.7579, 0.9883, 0.6281],\n",
      "        [1.4171, 0.8570, 0.7017],\n",
      "        [0.1624, 0.2367, 1.3165]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "y = torch.rand(5, 3)\n",
    "\n",
    "print(f\"x + y = \\n{x + y}\")\n",
    "\n",
    "print(f\"x = \\n{x.add_(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6319, 0.4041, 0.3563],\n",
      "        [0.4245, 0.3924, 0.8710],\n",
      "        [0.0683, 0.2766, 0.8291],\n",
      "        [0.9181, 0.5231, 0.2561],\n",
      "        [0.7665, 0.9804, 0.7721]])\n"
     ]
    }
   ],
   "source": [
    "print(torch.rand_like(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 1st row of x:\n",
      "tensor([1.4379, 1.0812, 1.1456])\n"
     ]
    }
   ],
   "source": [
    "# numpy-likeにスライスすることも可能\n",
    "print(f\"The 1st row of x:\\n{x[0,:]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チュートリアルでは紹介しきれない演算子が多くある。\n",
    "(例えば、transposeやslicing、代数演算など)\n",
    "\n",
    "それらの詳細は以下にすべてまとまっているので、適宜参考にすること。\n",
    "\n",
    "[docs](https://pytorch.org/docs/stable/torch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy-converted obj: [1. 1. 1. 1. 1.]\n",
      "the converted obj reference the same obejct.\n",
      "numpy-converted obj: [2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "# tensorをndarrayに変換可能\n",
    "\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "print(f\"numpy-converted obj: {b}\")\n",
    "\n",
    "a.add_(1)\n",
    "\n",
    "print(f\"the converted obj reference the same obejct.\")\n",
    "print(f\"numpy-converted obj: {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "\n",
    "参考ページ:[ここ](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "# out = z.mean()\n",
    "\n",
    "# print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記のように`requires_grad`属性を`True`にしたいtensorに対して演算を行うと、\n",
    "その演算に対する微分情報が得られる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上の結果は、`out`を`x`で偏微分したもの、つまりはヤコビアン。\n",
    "(`out`は`x`の関数。)\n",
    "\n",
    "計算過程を詳説すると、以下のとおり。\n",
    "\n",
    "---\n",
    "`out`を$\\boldsymbol{o}$ と表記すると、$\\boldsymbol{o} = \\frac{1}{4} \\sum 3(\\boldsymbol{x} +2)^2$となる。\n",
    "\n",
    "これを$\\boldsymbol{x}$ で微分したものが上記となる。\n",
    "\n",
    "$\\frac{\\boldsymbol{o}}{x_i} = \\frac{3}{2}(x_i + 2)$である。$x_i = 1$のとき、これは4.5となる。\n",
    "\n",
    "よって、上記のような計算結果となる。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8., 8.],\n",
      "        [8., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = 2 * x ** 4 + 5\n",
    "\n",
    "y.backward(torch.ones(2, 2))\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ベクトル$\\boldsymbol{y}$に対して、ベクトル$\\boldsymbol{x}$の各要素で偏微分したものはヤコビ行列(ヤコビアン)と呼ばれる。\n",
    "\n",
    "一般に、`torch.autograd`はヤコビアンと行列の積を求める計算エンジンである。\n",
    "\n",
    "つまり、任意のベクトル$\\boldsymbol{v}$が与えられた場合、$\\boldsymbol{v}^\\mathrm{T} \\cdot \\boldsymbol{J}$が計算される。\n",
    "\n",
    "仮に$\\boldsymbol{v}$がスカラー量$l = f(\\boldsymbol{y})$の勾配であった場合、つまり、\n",
    "$\\boldsymbol{v} = \\{ \\frac{\\partial{l}}{\\partial{y_1}},...,  \\frac{\\partial{l}}{\\partial{y_n}}\\}^\\mathrm{T}$\n",
    "である場合、**連鎖律(chain rule)** によって、\n",
    "ベクトル-ヤコビアンの積は、スカラー量$l$を$\\boldsymbol{x}$で微分したものとなる。\n",
    "\n",
    "$$\n",
    "\\boldsymbol{J}^\\mathrm{T} \\cdot \\boldsymbol{v} = \\{ \\frac{\\partial{l}}{\\partial{x_1}},...,  \\frac{\\partial{l}}{\\partial{x_m}}\\}^\\mathrm{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワーク\n",
    "\n",
    "Pytorchでは`torch.nn`パッケージを用いてニューラルネットワークを構築していく。\n",
    "\n",
    "ニューラルネットワークのトレーニングの主な流れは以下のとおり。\n",
    "\n",
    "- ネットワークを構築する\n",
    "- 入力データを入れる\n",
    "- 損失を計算する\n",
    "- backprop計算\n",
    "- ネットワーク内の重みを更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # 1層目\n",
    "        # 1チャンネルの画像データを入力\n",
    "        # 出力には6チェンネルのデータとなる\n",
    "        # カーネルサイズは3x3\n",
    "        # 注: 畳み込み層では出力画像のサイズを指定しない。\n",
    "        # (サイズはカーネルサイズやPoolong層、パディングによって自動的に決まる)\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        \n",
    "        # 2層目\n",
    "        # 6チャンネルのデータを入力\n",
    "        # 出力には16チェンネルのデータとなる\n",
    "        # カーネルサイズは3x3\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        \n",
    "        # 3層目以降は全結合層(アフィン層とも)\n",
    "        # 入力は16チャンネルの6x6の２2次元データで、\n",
    "        # 120行の出力を出す\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
    "        # 以降は同様\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Max pooling8(2x2)をかます\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # Pooling範囲が正方であるならば数字単独でもOK\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # フラット化する\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        # あとは残りのAffine層に突っ込む\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        # all dimensions except the batch dimension\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`forward()`を定義したが、`backward`は定義していない。\n",
    "\n",
    "これについては、`forward`の処理から自動的に決まるので、\n",
    "あとは`autograd`がやってくれる。\n",
    "\n",
    "ユーザーは好きに`forward()`内の演算を定義するだけでよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parameters()`で重みを取得できる。\n",
    "\n",
    "以下では試しに1層目の重みのサイズをprintしている。\n",
    "\n",
    "1チャンネルの画像データに対して、(3x3)のカーネルを適用して、\n",
    "6チャンネルのデータを出力する重みとなっていることがわかる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0278,  0.1053,  0.1006, -0.1327, -0.0275,  0.1336,  0.0397, -0.0255,\n",
      "          0.0373, -0.2106]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 適当な値で値を出力してみる。\n",
    "data_input = torch.randn(1, 1, 32, 32)\n",
    "data_out = net(data_input)\n",
    "print(data_out)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
